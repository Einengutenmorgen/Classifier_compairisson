{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678092c2",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f827f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea5aa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17529\n",
      "75\n",
      "['H' '403' '201.1' '411' '402' '410' '704' '505' '701' '504' '305.1' '110'\n",
      " '601.1' '401' '506' '605.1' '703.1' '303' '407' '603' '606.1' '202.1'\n",
      " '106' '503' '414' '412' '601.2' '109' '406' '304' '000' '608.2' '501'\n",
      " '201.2' '203' '301' '107' '204' '416.2' '502' '413' '103.1' '105' '104'\n",
      " '705' '108' '602.2' '608.1' '202.4' '202.2' '604' '706' '302' '507'\n",
      " '602.1' '409' '605.2' '415' '101' '607.2' '405' '404' '416.1' '408'\n",
      " '607.1' '703.2' '606.2' '305.6' '102' '305.2' '305.3' '202.3' '702'\n",
      " '607.3' '103.2']\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['text', 'text_en', 'cmp_code', 'eu_code', 'party', 'domain'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/christophhau/Desktop/HA_ML/data/aggregated/german_manifestos_combined.csv'\n",
    "\n",
    "df= pd.read_csv(path)\n",
    "print(len(df))\n",
    "print(len(df['cmp_code'].unique())) #labels und text_en\n",
    "print(df['cmp_code'].unique()) #labels und text_en\n",
    "print(df['text_en'].isna().sum())\n",
    "print(df['text_en'].isna().sum())\n",
    "print(df['cmp_code'].isna().sum())\n",
    "\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608d666",
   "metadata": {},
   "source": [
    "# formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf75754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17525\n",
      "71\n",
      "71\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 12267\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2626\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2626\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/christophhau/Desktop/HA_ML/data/aggregated/german_manifestos_combined.csv'\n",
    "df= pd.read_csv(path)\n",
    "\n",
    "\n",
    "df_small = df[[\"text_en\", \"cmp_code\"]].rename(columns={\"text_en\": \"text\", \"cmp_code\": \"label\"})\n",
    "df_small = df_small.groupby(\"label\").filter(lambda x: len(x) > 1)\n",
    "print(len(df_small))\n",
    "print(len(df_small['label'].unique())) #labels und text_en\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_small[\"label\"] = le.fit_transform(df_small[\"label\"])\n",
    "print(len(df_small['label'].unique())) #labels und text_en\n",
    "\n",
    "train_df, test_df = train_test_split(df_small, test_size=0.30, random_state=42, stratify=df_small[\"label\"])\n",
    "test_df = test_df.groupby(\"label\").filter(lambda x: len(x) > 1)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42, stratify=test_df[\"label\"])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"val\": val_dataset\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ef6fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 12267/12267 [00:01<00:00, 7532.54 examples/s]\n",
      "Map: 100%|██████████| 2626/2626 [00:00<00:00, 9071.00 examples/s]\n",
      "Map: 100%|██████████| 2626/2626 [00:00<00:00, 9048.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=71)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"manifesto_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c704653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4602' max='4602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4602/4602 1:40:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.944600</td>\n",
       "      <td>1.795486</td>\n",
       "      <td>0.541127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.308200</td>\n",
       "      <td>1.580742</td>\n",
       "      <td>0.600533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>1.584514</td>\n",
       "      <td>0.619573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophhau/Desktop/HA_ML/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/christophhau/Desktop/HA_ML/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/christophhau/Desktop/HA_ML/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/christophhau/Desktop/HA_ML/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/christophhau/Desktop/HA_ML/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4602, training_loss=1.4874885273932996, metrics={'train_runtime': 6063.1904, 'train_samples_per_second': 6.07, 'train_steps_per_second': 0.759, 'total_flos': 9688748636132352.0, 'train_loss': 1.4874885273932996, 'epoch': 3.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"Saving model and tokenizer...\")\n",
    "trainer.save_model(\"./manifesto_classifier_sentence\")\n",
    "tokenizer.save_pretrained(\"./manifesto_classifier_context\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251404c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(dataset[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016eda0",
   "metadata": {},
   "source": [
    "# context classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484a8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Zeile 0 ---\n",
      "\n",
      "\u001b[1mContext:\u001b[0m\n",
      "TIME FOR PROSPERITY SOCIAL MARKET ECONOMY AND HEALTH SOCIAL MARKET ECONOMY We are firmly committed to the principles of the social market economy, which have ensured prosperity and social peace in our country for decades.\n",
      "--------------------\n",
      "\n",
      "\u001b[1mText (mit umliegenden Zeilen):\u001b[0m\n",
      "> \u001b[1mTIME FOR PROSPERITY SOCIAL MARKET ECONOMY AND HEALTH\u001b[0m\n",
      "  SOCIAL MARKET ECONOMY\n",
      "  We are firmly committed to the principles of the social market economy, which have ensured prosperity and social peace in our country for decades.\n",
      "  In a time of global challenges, we want to preserve the fundamental values of our economic order\n",
      "--- Zeile 1 ---\n",
      "\n",
      "\u001b[1mContext:\u001b[0m\n",
      "TIME FOR PROSPERITY SOCIAL MARKET ECONOMY AND HEALTH SOCIAL MARKET ECONOMY We are firmly committed to the principles of the social market economy, which have ensured prosperity and social peace in our country for decades.\n",
      "--------------------\n",
      "\n",
      "\u001b[1mText (mit umliegenden Zeilen):\u001b[0m\n",
      "  TIME FOR PROSPERITY SOCIAL MARKET ECONOMY AND HEALTH\n",
      "> \u001b[1mSOCIAL MARKET ECONOMY\u001b[0m\n",
      "  We are firmly committed to the principles of the social market economy, which have ensured prosperity and social peace in our country for decades.\n",
      "  In a time of global challenges, we want to preserve the fundamental values of our economic order\n",
      "  and develop them further in such a way that we can guarantee a free and successful society in which citizens can lead their lives independently and self-determined.\n",
      "\n",
      "Alle Zeilen wurden untersucht.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Erstellen eines Beispiel-DataFrames zur Demonstration\n",
    "# Ersetze diesen Teil durch das Laden deiner eigenen CSV-Datei\n",
    "df = pd.read_csv('data/processed/german_manifestos_punctuation_context.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Zeilenweise durch den DataFrame iterieren\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"--- Zeile {index} ---\")\n",
    "    \n",
    "    # 1. Den Inhalt der Spalte 'context' für die aktuelle Zeile ausgeben\n",
    "    print(\"\\n\\033[1mContext:\\033[0m\") # Fettgedruckte Überschrift\n",
    "    print(row['context'])\n",
    "    print(\"-\" * 20) # Trennlinie\n",
    "\n",
    "    # 2. Den Inhalt der Spalte 'text' mit den umliegenden Zeilen anzeigen\n",
    "    print(\"\\n\\033[1mText (mit umliegenden Zeilen):\\033[0m\") # Fettgedruckte Überschrift\n",
    "    \n",
    "    # Start- und End-Index für den Ausschnitt festlegen\n",
    "    # max() und min() verhindern Fehler am Anfang und Ende des DataFrames\n",
    "    start = max(0, index - 3)\n",
    "    end = min(len(df), index + 4)\n",
    "    \n",
    "    # Den relevanten Ausschnitt des DataFrames ausgeben\n",
    "    # Wir heben die aktuelle Zeile hervor\n",
    "    for i in range(start, end):\n",
    "        if i == index:\n",
    "            # Aktuelle Zeile mit > markieren und fett drucken\n",
    "            print(f\"> \\033[1m{df.loc[i, 'text_en']}\\033[0m\")\n",
    "        else:\n",
    "            print(f\"  {df.loc[i, 'text_en']}\")\n",
    "\n",
    "    # 3. Auf eine Benutzereingabe warten, um zur nächsten Zeile zu gelangen\n",
    "    # Das Skript pausiert hier, bis du \"Enter\" drückst.\n",
    "    try:\n",
    "        inp =input(\"\\nDrücke Enter, um zur nächsten Zeile zu gelangen (oder Strg+C zum Abbrechen)...\")\n",
    "        if inp == 'exit':\n",
    "            break# IPython.display.clear_output() # Diese Zeile einkommentieren, um die Ausgabe nach jeder Iteration zu löschen\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSchleife wurde vom Benutzer abgebrochen.\")\n",
    "        break # Die Schleife beenden\n",
    "\n",
    "print(\"\\nAlle Zeilen wurden untersucht.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d095b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17884, 18)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Verzeichnis mit den CSV-Dateien\n",
    "csv_dir = 'data/contextaware_data_20250927_112401'\n",
    "\n",
    "# Alle CSV-Dateipfade im Verzeichnis sammeln\n",
    "df_paths = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# DataFrames einlesen und kombinieren\n",
    "dfs = [pd.read_csv(path) for path in df_paths]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Kombinierten DataFrame als neue CSV speichern\n",
    "combined_df.to_csv('/Users/christophhau/Desktop/HA_ML/data/contextaware_data_20250927_112401/combined_output.csv', index=False)\n",
    "\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82540968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens already in tokenizer vocabulary\n",
      "Dataset created: 12267 train, 2626 val, 2626 test\n",
      "Example input: <s> not make private motorized transport unaffordable. </s> </s> We want to create acceptance through transparency and dialog at eye level, e.g. for o...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 12267\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2626\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2626\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "path = 'data/contextaware_data_20250927_112401/combined_output_context.csv'\n",
    "def create_formatted_input(text, context, tokenizer, max_tokens=200):\n",
    "    \"\"\"Create sentence-pair input: <s> text </s> </s> context </s>\"\"\"\n",
    "    # Truncate context to max_tokens\n",
    "    context_tokens = tokenizer.tokenize(str(context))[:max_tokens]\n",
    "    truncated_context = tokenizer.convert_tokens_to_string(context_tokens)\n",
    "    \n",
    "    return f\"<s> {str(text).strip()} </s> </s> {truncated_context} </s>\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Load tokenizer for precise token counting\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "special_tokens = ['<s>', '</s>']\n",
    "new_tokens = [token for token in special_tokens if token not in tokenizer.vocab]\n",
    "\n",
    "if new_tokens:\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Added {len(new_tokens)} new tokens to tokenizer: {new_tokens}\")\n",
    "else:\n",
    "    print(\"Special tokens already in tokenizer vocabulary\")\n",
    "\n",
    "# Create formatted inputs\n",
    "df['text'] = df.apply(lambda row: create_formatted_input(\n",
    "    row['text_en'], row['context'], tokenizer), axis=1)\n",
    "\n",
    "# Prepare dataset (same as your original code)\n",
    "df_small = df[[\"text\", \"cmp_code\"]].rename(columns={\"cmp_code\": \"label\"})\n",
    "df_small = df_small.groupby(\"label\").filter(lambda x: len(x) > 1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_small[\"label\"] = le.fit_transform(df_small[\"label\"])\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df_small, test_size=0.30, random_state=42, stratify=df_small[\"label\"])\n",
    "test_df = test_df.groupby(\"label\").filter(lambda x: len(x) > 1)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42, stratify=test_df[\"label\"])\n",
    "\n",
    "# Create datasets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "    \"val\": Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "})\n",
    "\n",
    "print(f\"Dataset created: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")\n",
    "print(f\"Example input: {train_df.iloc[0]['text'][:150]}...\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92a26e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Apple Silicon GPU) ist verfügbar und wird verwendet\n"
     ]
    }
   ],
   "source": [
    "# MPS Device Check und Setup\n",
    "import torch\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"✅ MPS (Apple Silicon GPU) ist verfügbar und wird verwendet\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"✅ CUDA GPU wird verwendet\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"⚠️ Nur CPU verfügbar - Training wird langsamer sein\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b9e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 12267/12267 [00:01<00:00, 7764.14 examples/s]\n",
      "Map: 100%|██████████| 2626/2626 [00:00<00:00, 8167.59 examples/s]\n",
      "Map: 100%|██████████| 2626/2626 [00:00<00:00, 8079.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=71)\n",
    "model.to(device)\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"manifesto_classifier\",\n",
    "    \n",
    "    # MPS-optimierte Einstellungen\n",
    "    per_device_train_batch_size=4,  # Kleinere Batch Size für MPS\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # Kompensiert kleinere Batch Size\n",
    "    \n",
    "    # Training Parameter\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Evaluation und Saving\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    # MPS-spezifische Einstellungen\n",
    "    dataloader_pin_memory=False,  # Wichtig für MPS\n",
    "    fp16=False,  # MPS unterstützt noch kein FP16\n",
    "    push_to_hub=False,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    \n",
    "    # Warmup für bessere Konvergenz\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5213fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"Saving model and tokenizer...\")\n",
    "trainer.save_model(\"./manifesto_classifier_sentence\")\n",
    "tokenizer.save_pretrained(\"./manifesto_classifier_context\")\n",
    "\n",
    "\n",
    "print('saved')\n",
    "trainer.evaluate(dataset[\"val\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
